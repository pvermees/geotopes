rownames(chi2dat) <- c('sample 1','sample 2')
#12
nrdat <- 10
maxx <- 100
rxp <- 150
ra <- 10
rb <- 5
sy <- 5
err <- rnorm(nrdat,mean=0,sd=sy)
rx <- sort(signif(maxx*runif(nrdat),2))
ry <- signif(ra + rb * rx + err,2)
rdat <- rbind(rx,ry)
rownames(rdat) <- c('x','y')
colnames(rdat) <- 1:nrdat
#15
ncs <- 5
lr <- MASS::mvrnorm(ncs,mu=c(1,0,-1),Sigma=diag(3)/10)
comp <- matrix(signif(100*geostats::clr(lr,inverse=TRUE),3),ncol=3)
colnames(comp) <- c('X','Y','Z')
rownames(comp) <- 1:ncs
#16
NvNn <- 40
E <- 2/3
Pv <- 1/(1+1/(1-E))
item <- item+1
# Under the null hypothesis, the rejection region for
# the number of 1s includes all values smaller than
min1s <- qbinom(p=0.05, size=100, prob=1/6, lower.tail = TRUE)
min1s
# Under the alternative hypothesis, the probability
# of throwing a 1 more than min1s times is
pbinom(q=min1s, size=100, prob=1/7, lower.tail=FALSE)
item <- item+1
# This is an application of Bayes' Rule:
Ppyrite <- 0.1
Pgold <- 0.001
Ppyrite_if_gold <- 0.8
Pgold_if_pyrite <- Ppyrite_if_gold*Pgold/Ppyrite
Pgold_if_pyrite
item <- item+1
ppois(q=2,lambda=2,lower.tail=FALSE)
qpois(p=0.95,lambda=2,lower.tail=TRUE)
item <- item+1
fitem <- fitem+1
oldpar <- par(mfrow=c(1,2))
d <- c(0,0.1,0.2,0.1,0.2,0.3,0.1)
nd <- length(d)
barplot(d[-1],col=c(rep('white',4),rep('black',3)),axes=FALSE)
plot(1:nd,cumsum(d),type='s',bty='n',axes=FALSE,ann=FALSE,ylim=c(0,1))
Axis(side=2,at=seq(from=0,to=1,by=0.1))
grid(lwd=2,col='grey50')
par(oldpar)
# answer: 1 - 0.6 = 0.4
item <- item+1
fitem <- fitem+1
plotpdf <- function(x,d){
plot(x,d,type='l',bty='n',ann=FALSE,axes=FALSE)
Axis(side=1,at=c(0,0.5,1))
}
plotcdf <- function(x,d){
dx <- diff(x)
foo <- cumsum(d*c(dx[1],dx))
cdf <- foo/tail(foo,n=1)
plot(x,cdf,type='l',bty='n',ann=FALSE,axes=FALSE)
Axis(side=1,at=c(0,0.5,1))
}
dx <- 1e-4
x <- list()
pd <- list()
x[[1]] <- seq(from=0,to=1,length.out=100)
foo <- c(x[[1]][1:50],x[[1]][50:1])
pd[[1]] <- foo/sum(foo)
x[[2]] <- c(0,seq(from=dx,to=1-dx,length.out=100),1)
foo <- c(0,x[[2]][52:2],x[[2]][3:51],0)
pd[[2]] <- foo/sum(foo)
x[[3]] <- c(0,
seq(from=dx,to=0.5,length.out=49),
seq(from=0.5+dx,to=1-dx,length.out=49),
1)
foo <- c(0,rep(1/3,49),rep(2/3,49),0)
pd[[3]] <- foo/sum(foo)
x[[4]] <- c(0,
seq(from=dx,to=0.5,length.out=49),
seq(from=0.5+dx,to=1-dx,length.out=49),
1)
foo <- c(0,rep(2/3,49),rep(1/3,49),0)
pd[[4]] <- foo/sum(foo)
x[[5]] <- c(0,dx,
seq(from=2*dx,to=0.5-dx,length.out=50),
0.5,
seq(from=0.5+dx,to=1-2*dx,length.out=50),
1-dx,1)
foo <- c(0,1,
rep(0,50),
1,
rep(0,50),
1,0)
pd[[5]] <- foo/sum(foo)
x[[6]] <- x[[5]]
foo <- c(0,0,
rep(0,50),
1,
rep(0,50),
1,0)
pd[[6]] <- foo/sum(foo)
i <- sample(6)
j <- sample(6)
pdlabs <- c('a)','b)','c)')
cdlabs <- c('1)','2)','3)','4)','5)','6)')
oldpar <- par(mfrow=c(2,6),mar=c(2,3,1,1))
for (jj in 1:3){
jjj <- j[jj]
plotpdf(x[[jjj]],pd[[jjj]])
legend('topleft',legend=pdlabs[jj],bty='n',adj=c(3,-1),xpd=NA,cex=1.2)
}
plot.new()
plot.new()
plot.new()
for (ii in 1:6){
iii <- i[ii]
plotcdf(x[[iii]],pd[[iii]])
legend('topleft',legend=cdlabs[ii],bty='n',adj=c(3,-1),xpd=NA,cex=1.2)
}
par(oldpar)
sol <- rep(0,3)
for (ii in 1:3) sol[ii] <- which(i%in%j[ii])
sol
item <- item+1
print(c(mu,sigma,norminbound,normaxbound))
pnorm(q=normaxbound, mean=mu, sd=sigma) - pnorm(q=norminbound, mean=mu, sd=sigma)
item <- item+1
26^2 * 10^2 * 26^3
item <- item+1
# define variables
x <- 2
sx <- 1
y <- 3
sy <- 5
# estimate z
z <- 4 + 2 * log(x + 1) - 5 * y^2
# rewrite z as: z = a + b - c, then the uncertainty of a is
sa <- 0
# the uncertainty of b is
sb <- 2*sx/(x+1)
# c is
cc <- 5*y^2
# the uncertainty of c is
sc <- cc*2*sy/y
# hence the uncertainty of z is
sz <- sqrt(sa^2+sb^2+sc^2)
c(z,sz)
item <- item+1
dbinom(x=3, size=7, prob=0.5)
dbinom(x=3, size=7, prob=0.7)
item <- item+1
titem <- titem+1
knitr::kable(tdat,col.names=1:nnum,caption=paste0('Data for question ',item,'.'))
print(tdat)
ci <- t.test(tdat)$conf.int
ci
# the null hypothesis is
print(t.test(tdat,mu=65)$p.value>=0.05)
item <- item+1
titem <- titem+1
knitr::kable(chi2dat,caption=paste0('Data for question ',item,'.'))
print(chi2dat)
chisq.test(x=chi2dat)$p.value
# the p-value is greater than 0.05, hence we are unable to reject the null hypothesis
item <- item+1
titem <- titem+1
knitr::kable(rdat,caption=paste0('Data for question ',item,'.'))
print(rdat)
x <- rdat[1,]
y <- rdat[2,]
fit <- lm(y ~ x)
coef(fit) # a)
y150 <- predict(fit,newdata=data.frame(x=150),interval='prediction')
y150 # b and c)
item <- item+1
fitem <- fitem+1
oldpar <- par(mar=c(4,3,2,2))
vars <- c('Zn','Cu','Pb','Ti','Sr')
nv <- length(vars)
ns <- 10
E <- rWishart(1,nv,diag(nv))[,,1]
pcdat <- MASS::mvrnorm(n=ns,mu=rnorm(nv,mean=0,sd=3),Sigma=E)
colnames(pcdat) <- vars
rownames(pcdat) <- 1:ns
pc <- prcomp(pcdat)
biplot(pc)
par(oldpar)
# example (not applicable to this instance):
# 2, 8 and 9 are similar because they plot close together,
# so are 3 and 7, 1 and 10, and 5 and 6.
# 3 and 7 are enriched in Cu and Zn, 1 and 10 in Sr and Pb
# 4, 6 and 6 are enriched in Ti, 2, 8 and 9 are depleted in those elements
# Sr and Pb are correlated with each other
# Cu and Sr are anti-correlated, and so are Zn and Pb
# Ti and Cu are independent and so are Ti and Sr
item <- item+1
# QDA is more accurate because it has more parameters to describe the data.
# LDA is more precise because it has fewer parameters.
# For small samples, LDA is the only viable option. We need a lot of data
# to fit all the extra parameters for QDA.
item <- item+1
titem <- titem+1
knitr::kable(comp,caption=paste0('Data for question ',item,'.'))
print(comp)
# logratio transformation:
lr <- geostats::alr(comp)
lrm <- colMeans(lr)
# inverse logratio transformation:
lrm <- geostats::alr(lrm,inverse=TRUE)
signif(100*lrm,3)
item <- item+1
# Nv is a binomial variable with sample size Nv + Nn and parameter p,
# where p = Pv/(Pn+Pv) = 1/(1+Pn/Pv) = 1/(1+1/(1-E))
# Applying a binomial test
E <- 0.5
p <- 1/(1+1/(1-E))
binom.test(x=10,n=10+30,p=p,alternative='less')$p.value
binom.test(x=20,n=20+60,p=p,alternative='less')$p.value
binom.test(x=30,n=30+90,p=p,alternative='less')$p.value
# Discussion: the power of the binomial test to reject
# the null hypothesis increases with sample size
# The size of clinical trials is calculated beforehand.
chisq.test(x=chi2dat)
x<-2;y<-3;z<-4+2*log(x+1)-5*(y^2);print(z);Sx<-1;Sy<-5;dzx<-2/3;dzy<-(-30);Sz<-sqrt((dzx^2*Sx^2)+(dzy^2*Sy^2))
Sz
data<-cbind(X=c(75.6,60.3,54.0,76.7,64,2),Y=c(18.7,30.5,35,4,15.9,23.8),Z=c(5.69,9.20,10.50,7.4,12.10))
d<-cbind(X=c(75.6,60.3,54.0,76.7,64,2),Y=c(18.7,30.5,35,4,15.9,23.8),Z=c(5.69,9.20,10.50,7.4,12.10))
d
dataclr<-clr(data)
dataclr<-geostats::clr(data)
Clrm<-exp(colMeans(dataclr))
Clrm
12.247^2
x=c(1.7,5,31,42,45,50,68,73,74,78);y=c(13,33,170,210,230,260,350,280,380,400)
lm(y~x)
knitr::opts_chunk$set(include=TRUE, echo=TRUE)
set.seed(31)
solution <- FALSE
item <- 0
fitem <- 0
titem <- 0
#4
Ppyrite <- 0.1
Pgold <- 0.001
Ppyrite_if_gold <- 0.8
#5
pHead <- 0.7 # probability of head for biased coin
nHead <- 3
nTail <- 4
#6
invpdie <- 7
#7
nlightning <- 2
lightningpercentile <- 95
#8
mu <- 3
sigma <- 2
norminbound <- -1
normaxbound <- 2
#9
aa <- 4
bb <- 2
cc <- 1
dd <- 5
ee <- 2
xx <- 2
sx <- 1
yy <- 3
sy <- 5
#10
nnum <- 10
tdat <- matrix(signif(rnorm(n=nnum, mean=50, sd=20),3),1,nnum)
#11
chi2dat <- rbind(c(20,11,60),
c(10,15,50))
colnames(chi2dat) <- c('quartz','feldspar','lithics')
rownames(chi2dat) <- c('sample 1','sample 2')
#12
nrdat <- 10
maxx <- 100
rxp <- 150
ra <- 10
rb <- 5
sy <- 5
err <- rnorm(nrdat,mean=0,sd=sy)
rx <- sort(signif(maxx*runif(nrdat),2))
ry <- signif(ra + rb * rx + err,2)
rdat <- rbind(rx,ry)
rownames(rdat) <- c('x','y')
colnames(rdat) <- 1:nrdat
#15
ncs <- 5
lr <- MASS::mvrnorm(ncs,mu=c(1,0,-1),Sigma=diag(3)/10)
comp <- matrix(signif(100*geostats::clr(lr,inverse=TRUE),3),ncol=3)
colnames(comp) <- c('X','Y','Z')
rownames(comp) <- 1:ncs
#16
NvNn <- 40
E <- 2/3
Pv <- 1/(1+1/(1-E))
item <- item+1
fitem <- fitem+1
oldpar <- par(mfrow=c(1,2))
d <- c(0,0.1,0.2,0.1,0.2,0.3,0.1)
nd <- length(d)
barplot(d[-1],col=c(rep('white',4),rep('black',3)),axes=FALSE)
plot(1:nd,cumsum(d),type='s',bty='n',axes=FALSE,ann=FALSE,ylim=c(0,1))
Axis(side=2,at=seq(from=0,to=1,by=0.1))
grid(lwd=2,col='grey50')
par(oldpar)
# answer: 1 - 0.6 = 0.4
item <- item+1
# Under the null hypothesis, the rejection region for
# the number of 1s includes all values smaller than
min1s <- qbinom(p=0.05, size=100, prob=1/6, lower.tail = TRUE)
min1s
# Under the alternative hypothesis, the probability
# of throwing a 1 more than min1s times is
pbinom(q=min1s, size=100, prob=1/7, lower.tail=FALSE)
item <- item+1
dbinom(x=3, size=7, prob=0.5)
dbinom(x=3, size=7, prob=0.7)
item <- item+1
# This is an application of Bayes' Rule:
Ppyrite <- 0.1
Pgold <- 0.001
Ppyrite_if_gold <- 0.8
Pgold_if_pyrite <- Ppyrite_if_gold*Pgold/Ppyrite
Pgold_if_pyrite
item <- item+1
# define variables
x <- 2
sx <- 1
y <- 3
sy <- 5
# estimate z
z <- 4 + 2 * log(x + 1) - 5 * y^2
# rewrite z as: z = a + b - c, then the uncertainty of a is
sa <- 0
# the uncertainty of b is
sb <- 2*sx/(x+1)
# c is
cc <- 5*y^2
# the uncertainty of c is
sc <- cc*2*sy/y
# hence the uncertainty of z is
sz <- sqrt(sa^2+sb^2+sc^2)
c(z,sz)
item <- item+1
print(c(mu,sigma,norminbound,normaxbound))
pnorm(q=normaxbound, mean=mu, sd=sigma) - pnorm(q=norminbound, mean=mu, sd=sigma)
item <- item+1
26^2 * 10^2 * 26^3
item <- item+1
fitem <- fitem+1
plotpdf <- function(x,d){
plot(x,d,type='l',bty='n',ann=FALSE,axes=FALSE)
Axis(side=1,at=c(0,0.5,1))
}
plotcdf <- function(x,d){
dx <- diff(x)
foo <- cumsum(d*c(dx[1],dx))
cdf <- foo/tail(foo,n=1)
plot(x,cdf,type='l',bty='n',ann=FALSE,axes=FALSE)
Axis(side=1,at=c(0,0.5,1))
}
dx <- 1e-4
x <- list()
pd <- list()
x[[1]] <- seq(from=0,to=1,length.out=100)
foo <- c(x[[1]][1:50],x[[1]][50:1])
pd[[1]] <- foo/sum(foo)
x[[2]] <- c(0,seq(from=dx,to=1-dx,length.out=100),1)
foo <- c(0,x[[2]][52:2],x[[2]][3:51],0)
pd[[2]] <- foo/sum(foo)
x[[3]] <- c(0,
seq(from=dx,to=0.5,length.out=49),
seq(from=0.5+dx,to=1-dx,length.out=49),
1)
foo <- c(0,rep(1/3,49),rep(2/3,49),0)
pd[[3]] <- foo/sum(foo)
x[[4]] <- c(0,
seq(from=dx,to=0.5,length.out=49),
seq(from=0.5+dx,to=1-dx,length.out=49),
1)
foo <- c(0,rep(2/3,49),rep(1/3,49),0)
pd[[4]] <- foo/sum(foo)
x[[5]] <- c(0,dx,
seq(from=2*dx,to=0.5-dx,length.out=50),
0.5,
seq(from=0.5+dx,to=1-2*dx,length.out=50),
1-dx,1)
foo <- c(0,1,
rep(0,50),
1,
rep(0,50),
1,0)
pd[[5]] <- foo/sum(foo)
x[[6]] <- x[[5]]
foo <- c(0,0,
rep(0,50),
1,
rep(0,50),
1,0)
pd[[6]] <- foo/sum(foo)
i <- sample(6)
j <- sample(6)
pdlabs <- c('a)','b)','c)')
cdlabs <- c('1)','2)','3)','4)','5)','6)')
oldpar <- par(mfrow=c(2,6),mar=c(2,3,1,1))
for (jj in 1:3){
jjj <- j[jj]
plotpdf(x[[jjj]],pd[[jjj]])
legend('topleft',legend=pdlabs[jj],bty='n',adj=c(3,-1),xpd=NA,cex=1.2)
}
plot.new()
plot.new()
plot.new()
for (ii in 1:6){
iii <- i[ii]
plotcdf(x[[iii]],pd[[iii]])
legend('topleft',legend=cdlabs[ii],bty='n',adj=c(3,-1),xpd=NA,cex=1.2)
}
par(oldpar)
sol <- rep(0,3)
for (ii in 1:3) sol[ii] <- which(i%in%j[ii])
sol
item <- item+1
ppois(q=2,lambda=2,lower.tail=FALSE)
qpois(p=0.95,lambda=2,lower.tail=TRUE)
item <- item+1
titem <- titem+1
knitr::kable(tdat,col.names=1:nnum,caption=paste0('Data for question ',item,'.'))
print(tdat)
ci <- t.test(tdat)$conf.int
ci
# the null hypothesis is
print(t.test(tdat,mu=65)$p.value>=0.05)
item <- item+1
titem <- titem+1
knitr::kable(chi2dat,caption=paste0('Data for question ',item,'.'))
print(chi2dat)
chisq.test(x=chi2dat)$p.value
# the p-value is greater than 0.05, hence we are unable to reject the null hypothesis
item <- item+1
titem <- titem+1
knitr::kable(rdat,caption=paste0('Data for question ',item,'.'))
print(rdat)
x <- rdat[1,]
y <- rdat[2,]
fit <- lm(y ~ x)
coef(fit) # a)
y150 <- predict(fit,newdata=data.frame(x=150),interval='prediction')
y150 # b and c)
item <- item+1
fitem <- fitem+1
oldpar <- par(mar=c(4,3,2,2))
vars <- c('Zn','Cu','Pb','Ti','Sr')
nv <- length(vars)
ns <- 10
E <- rWishart(1,nv,diag(nv))[,,1]
pcdat <- MASS::mvrnorm(n=ns,mu=rnorm(nv,mean=0,sd=3),Sigma=E)
colnames(pcdat) <- vars
rownames(pcdat) <- 1:ns
pc <- prcomp(pcdat)
biplot(pc)
par(oldpar)
# example (not applicable to this instance):
# 2, 8 and 9 are similar because they plot close together,
# so are 3 and 7, 1 and 10, and 5 and 6.
# 3 and 7 are enriched in Cu and Zn, 1 and 10 in Sr and Pb
# 4, 6 and 6 are enriched in Ti, 2, 8 and 9 are depleted in those elements
# Sr and Pb are correlated with each other
# Cu and Sr are anti-correlated, and so are Zn and Pb
# Ti and Cu are independent and so are Ti and Sr
item <- item+1
# QDA is more accurate because it has more parameters to describe the data.
# LDA is more precise because it has fewer parameters.
# For small samples, LDA is the only viable option. We need a lot of data
# to fit all the extra parameters for QDA.
item <- item+1
titem <- titem+1
knitr::kable(comp,caption=paste0('Data for question ',item,'.'))
print(comp)
# logratio transformation:
lr <- geostats::alr(comp)
lrm <- colMeans(lr)
# inverse logratio transformation:
lrm <- geostats::alr(lrm,inverse=TRUE)
signif(100*lrm,3)
item <- item+1
# Nv is a binomial variable with sample size Nv + Nn and parameter p,
# where p = Pv/(Pn+Pv) = 1/(1+Pn/Pv) = 1/(1+1/(1-E))
# Applying a binomial test
E <- 0.5
p <- 1/(1+1/(1-E))
binom.test(x=10,n=10+30,p=p,alternative='less')$p.value
binom.test(x=20,n=20+60,p=p,alternative='less')$p.value
binom.test(x=30,n=30+90,p=p,alternative='less')$p.value
# Discussion: the power of the binomial test to reject
# the null hypothesis increases with sample size
# The size of clinical trials is calculated beforehand.
fit
y
x
sigma <-1;m0   <-1/6;m1   <-1/7;alpha <-0.05;crit <-qnorm(1-alpha, m0, sigma);pow <-pnorm(crit, m1, sigma, lower.tail=FALSE);beta <-1-pow
source('~/Documents/IsoplotR/generic.R')
source('~/Documents/IsoplotR/UPb.R')
